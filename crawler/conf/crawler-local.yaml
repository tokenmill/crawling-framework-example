fetcher.server.delay: 4.5
fetcher.server.min.delay: 3.0
fetcher.queue.mode: "byHost"
fetcher.threads.per.queue: 1
fetcher.threads.number: 5

partition.url.mode: "byHost"

metadata.track.path: false
metadata.track.depth: false
metadata.transfer:
 - "source"

http.agent.name: "Crawling Framework example"
http.agent.version: "1.0"
http.agent.description: "built with StormCrawler ${version}"
http.agent.url: "https://github.com/tokenmill/crawling-framework"
http.agent.email: "someone@someorganization.com"

http.accept.language: "en-us,en-gb,en;q=0.7,*;q=0.3"
http.accept: "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
http.content.limit: 1048576
http.store.responsetime: false
http.timeout: 30000

http.robots.403.allow: true

protocols: "http,https"
http.protocol.implementation: "com.digitalpebble.stormcrawler.protocol.httpclient.HttpProtocol"
https.protocol.implementation: "com.digitalpebble.stormcrawler.protocol.httpclient.HttpProtocol"

urlfilters.config.file: "urlfilters.json"

# revisit a page monthly (value in minutes)
fetchInterval.default: 44640

# revisit a page with a fetch error after 2 hours (value in minutes)
fetchInterval.fetch.error: 120

# revisit a page with an error every month (value in minutes)
fetchInterval.error: 44640

# Default implementation of Scheduler
scheduler.class: "com.digitalpebble.stormcrawler.persistence.DefaultScheduler"

topology.name: example-crawler
topology.workers: 1
topology.sleep.spout.wait.strategy.time.ms: 5000
topology.message.timeout.secs: 300
topology.max.spout.pending: 100
topology.debug: false

# ElasticSearch configuration
es.hostname: "127.0.0.1"
es.transport.port: 9300

es.urls.index.name: "cf-example-urls"
es.docs.index.name: "cf-example-docs"
es.httpsource.index.name: "cf-example-http_sources"
es.urls.doc.type: "url"
es.docs.doc.type: "doc"
es.httpsource.doc.type: "http_source"
